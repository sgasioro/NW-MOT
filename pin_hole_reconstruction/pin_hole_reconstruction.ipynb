{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ff2806c4-2de6-4917-95fd-579e3e1f5d57",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/projects/p30957/envs/pytorch-1.11-py38-alt-2/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import torch\n",
    "\n",
    "import gradoptics as optics\n",
    "from gradoptics.integrator import HierarchicalSamplingIntegrator\n",
    "from ml.siren import Siren\n",
    "\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "scene_objects = pickle.load(open(\"NW_mot_scene_components.pkl\", \"rb\"))\n",
    "\n",
    "# from simulation\n",
    "data_intensities_all = pickle.load(open(\"NW_mot_images.pkl\", \"rb\"))\n",
    "\n",
    "loss_file_name = 'loss_pixel_size_same.csv'\n",
    "\n",
    "targets = [];\n",
    "for img in data_intensities_all:\n",
    "    targets.append(img.flatten().cuda())\n",
    "\n",
    "sel_mask = []\n",
    "for img in targets:\n",
    "    sel_mask.append(torch.ones(img.shape, dtype=torch.bool))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3474cff2-0a56-49f2-8a7f-40f549f19028",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up SIREN model\n",
    "\n",
    "device = 'cuda'\n",
    "in_features = 3\n",
    "hidden_features = 256\n",
    "hidden_layers = 3\n",
    "out_features = 1\n",
    "\n",
    "model = Siren(in_features, hidden_features, hidden_layers, out_features,\n",
    "              outermost_linear=True, outermost_linear_activation=nn.ReLU()).double().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6aa66616-653c-4428-b829-9c27f7e9f4db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up scene for rendering in training\n",
    "# Region we want to integrate in + position\n",
    "rad = 0.03\n",
    "obj_pos = (0, 0, 0)\n",
    "\n",
    "light_source = optics.LightSourceFromNeuralNet(model, optics.BoundingSphere(radii=rad, \n",
    "                                                                     xc=obj_pos[0], yc=obj_pos[1], zc=obj_pos[2]),\n",
    "                                        rad=rad, x_pos=obj_pos[0], y_pos=obj_pos[1], z_pos=obj_pos[2])\n",
    "scene_train = optics.Scene(light_source)\n",
    "\n",
    "for obj in scene_objects:\n",
    "    scene_train.add_object(obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d407b175-39d6-4119-a7b6-68bf548404c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load neural net parameters from pre trained model\n",
    "pretrain_model = pickle.load(open(\"pre_trained_model_state_dict.pkl\", \"rb\"))\n",
    "model.load_state_dict(pretrain_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "efbab00f-0c93-475b-9f09-ca15627c23e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sensor_list = [obj for obj in scene_train.objects if type(obj) == optics.Sensor]\n",
    "lens_list = [obj for obj in scene_train.objects if type(obj) == optics.PerfectLens]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cf729e69-b784-4321-8181-6acd86ae7a22",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 512\n",
    "loss_fn = torch.nn.MSELoss()\n",
    "integrator = HierarchicalSamplingIntegrator(64, 64)\n",
    "optimizer = torch.optim.Adam(scene_train.light_source.network.parameters(), lr=2e-5)\n",
    "\n",
    "losses = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3363e4a6-3f8f-4235-9ff6-ffc4a92c20ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                               | 1/50000 [00:00<13:25:19,  1.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "82.2070606285163\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                              | 101/50000 [00:31<4:09:18,  3.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.03418725617606468\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|▏                                             | 201/50000 [01:01<4:07:37,  3.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.012460486703354132\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▎                                             | 301/50000 [01:31<4:06:51,  3.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.00568158061163318\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▎                                             | 401/50000 [02:01<4:05:49,  3.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.003585181330887282\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▍                                             | 501/50000 [02:31<4:08:15,  3.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0024363056865304693\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▌                                             | 601/50000 [03:01<4:06:33,  3.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0016789213670209203\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▋                                             | 701/50000 [03:31<4:04:17,  3.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0015053624097383328\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▋                                             | 724/50000 [03:38<4:08:16,  3.31it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 43\u001b[0m\n\u001b[1;32m     40\u001b[0m intensities \u001b[38;5;241m=\u001b[39m optics\u001b[38;5;241m.\u001b[39mbackward_ray_tracing(outgoing_rays,scene_train, scene_train\u001b[38;5;241m.\u001b[39mlight_source,integrator, max_iterations\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m6\u001b[39m)\n\u001b[1;32m     42\u001b[0m \u001b[38;5;66;03m# Scaling to help control loss values\u001b[39;00m\n\u001b[0;32m---> 43\u001b[0m im_scale \u001b[38;5;241m=\u001b[39m \u001b[43mtargets\u001b[49m\u001b[43m[\u001b[49m\u001b[43mcam\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43msel_mask\u001b[49m\u001b[43m[\u001b[49m\u001b[43mcam\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflatten\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mmean()\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m     45\u001b[0m intensities_normalized\u001b[38;5;241m.\u001b[39mappend(intensities\u001b[38;5;241m/\u001b[39mim_scale\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m5e5\u001b[39m)\n\u001b[1;32m     46\u001b[0m target_vals_normalized\u001b[38;5;241m.\u001b[39mappend(target_vals\u001b[38;5;241m.\u001b[39mdouble()\u001b[38;5;241m.\u001b[39mcuda()\u001b[38;5;241m/\u001b[39mim_scale)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for i_iter in tqdm(range(int(5e4))):\n",
    "    intensities_normalized = [];\n",
    "    target_vals_normalized = [];\n",
    "    for cam in range(len(targets)):\n",
    "        sensor_here = sensor_list[cam]\n",
    "        lens_here = lens_list[cam]\n",
    "        h_here, w_here = sensor_here.resolution\n",
    "\n",
    "        # Grab masked pixel indices + sample randomly\n",
    "        idxs_all = torch.cartesian_prod(torch.arange(h_here//2, -h_here//2, -1), \n",
    "                                        torch.arange(w_here//2, -w_here//2, -1))\n",
    "        \n",
    "        idxs_all = idxs_all[sel_mask[cam].flatten()]\n",
    "        \n",
    "        rand_pixels = torch.randint(0, len(idxs_all), (int(batch_size/len(targets)),))\n",
    "        target_vals = targets[cam][sel_mask[cam].flatten()][rand_pixels]  \n",
    "        \n",
    "        batch_pix_x = idxs_all[rand_pixels, 0]\n",
    "        batch_pix_y = idxs_all[rand_pixels, 1]\n",
    "\n",
    "        intensities_all = []\n",
    "        \n",
    "        device = 'cuda'\n",
    "        lens_pos = lens_list[cam].transform.transform[:-1, -1].to(device)\n",
    "        \n",
    "        nb_pixels = len(batch_pix_x)\n",
    "             \n",
    "        origins = torch.zeros((nb_pixels,3),device = device,dtype = torch.float64)\n",
    "        origins[:,0] = (batch_pix_x.to(device))* sensor_here.pixel_size[0]\n",
    "        origins[:,1] = (batch_pix_y.to(device))* sensor_here.pixel_size[1]\n",
    "        origins = sensor_here.c2w.apply_transform_(origins.reshape(-1, 3)).reshape((nb_pixels, 3))\n",
    "        #pA = 1 / (sensor_here.resolution[0] * sensor_here.pixel_size[0] * sensor_here.resolution[1] * sensor_here.pixel_size[1])\n",
    "        \n",
    "        \n",
    "        directions = lens_pos - origins\n",
    "        directions = directions/torch.norm(directions, dim=1, keepdim=True)  \n",
    "\n",
    "        \n",
    "        outgoing_rays = optics.Rays(origins, directions, device='cuda')\n",
    "        intensities = optics.backward_ray_tracing(outgoing_rays,scene_train, scene_train.light_source,integrator, max_iterations=6)\n",
    "\n",
    "        # Scaling to help control loss values\n",
    "        im_scale = targets[cam][sel_mask[cam].flatten()].mean().item()\n",
    "\n",
    "        intensities_normalized.append(intensities/im_scale*5e5)\n",
    "        target_vals_normalized.append(target_vals.double().cuda()/im_scale)\n",
    "        \n",
    "\n",
    "    # Calculate loss and update neural network parameters\n",
    "    loss = loss_fn(torch.cat(tuple(intensities_normalized)), torch.cat(tuple(target_vals_normalized)))\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    # Record and print out\n",
    "    losses.append(loss.item())\n",
    "    if i_iter % 100 == 0:\n",
    "        print(loss.item())\n",
    "        \n",
    "    if i_iter % 500 == 0:\n",
    "        with torch.no_grad():\n",
    "            torch.save(scene_train.light_source.network.state_dict(),f'model_{i_iter}_NW_MOT_all_cameras_long.pt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
